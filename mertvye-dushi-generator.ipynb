{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Загружаем текст книги\n",
        "with open('/content/mertvye-dushi.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Создаём уникальный набор символов\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Создаём словари символов\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Загружаем обученную модель\n",
        "model = tf.keras.models.load_model('/content/text_generation_model.h5')\n",
        "\n",
        "# Функция генерации текста\n",
        "def generate_text(model, start_string, num_generate=1000, temperature=0.6):\n",
        "    input_eval = [char2idx[s] for s in start_string if s in char2idx]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :]  # Берём предсказания для последнего символа\n",
        "\n",
        "        # Разделить logits на температуру\n",
        "        predictions /= temperature\n",
        "\n",
        "        # Выбираем следующий символ\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        # Обновляем входные данные\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Добавляем сгенерированный символ\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "        # Останавливаемся, если сгенерирован символ конца предложения\n",
        "        if idx2char[predicted_id] in {'.', '!', '?'}:\n",
        "            break\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Генерация текста\n",
        "start_string = \"Чичиков произнес: \"\n",
        "generated_text = generate_text(model, start_string, num_generate=70, temperature=0.8)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMEShAnm1XuB",
        "outputId": "d92bd94b-56ce-4d34-fc75-48bfe9aa2c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Чичиков произнес: «ни одного превосходительства представить себе, Несколько хранистка!\n"
          ]
        }
      ]
    }
  ]
}