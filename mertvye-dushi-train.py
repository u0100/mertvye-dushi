# -*- coding: utf-8 -*-
"""mertvye-dushi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hpOOR2OVDPHlEkKT8t5DKS9hYgfDrvH
"""

!pip install tensorflow

import tensorflow as tf
import numpy as np
import os

# Загрузка текстовых данных
file_path = '/content/mertvye-dushi.txt'  # Убедитесь, что файл загружен
text = open(file_path, 'r', encoding='utf-8').read()

# Создание словаря символов
vocab = sorted(set(text))
char2idx = {u: i for i, u in enumerate(vocab)}  # Символ -> индекс
idx2char = np.array(vocab)  # Индекс -> символ

# Преобразование текста в числовые последовательности
text_as_int = np.array([char2idx[c] for c in text])

# Создание последовательностей
seq_length = 100  # Длина последовательности
examples_per_epoch = len(text) // (seq_length + 1)

# Создание tf.data.Dataset
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

# Разделение на последовательности
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

# Функция для разделения на входные и целевые данные
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

# Применение функции к данным
dataset = sequences.map(split_input_target)

# Подготовка данных для обучения
BATCH_SIZE = 64
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Параметры модели
vocab_size = len(vocab)  # Размер словаря
embedding_dim = 256  # Размерность embedding-слоя
rnn_units = 1024  # Количество нейронов в GRU

# Создание модели
def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=None),  # Убрали batch_input_shape
        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])
    return model

# Создание модели
model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)

# Компиляция модели
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

# Обучение модели
EPOCHS = 100  # Количество эпох
history = model.fit(dataset, epochs=EPOCHS)

# Сохранение модели
model.save('text_generation_model.h5')