# -*- coding: utf-8 -*-
"""mertvye_dushi_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pq5lNtmkS0MecqtlPR323wnJUKplugbZ
"""

import tensorflow as tf
import numpy as np

# Загружаем текст книги
with open('/content/mertvye-dushi.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# Создаём уникальный набор символов
vocab = sorted(set(text))

# Создаём словари символов
char2idx = {u: i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

# Загружаем обученную модель
model = tf.keras.models.load_model('/content/text_generation_model.h5')

# Функция генерации текста
def generate_text(model, start_string, num_generate=1000, temperature=0.6):
    input_eval = [char2idx[s] for s in start_string if s in char2idx]
    input_eval = tf.expand_dims(input_eval, 0)

    text_generated = []

    for i in range(num_generate):
        predictions = model(input_eval)
        predictions = predictions[:, -1, :]  # Берём предсказания для последнего символа

        # Разделить logits на температуру
        predictions /= temperature

        # Выбираем следующий символ
        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()

        # Обновляем входные данные
        input_eval = tf.expand_dims([predicted_id], 0)

        # Добавляем сгенерированный символ
        text_generated.append(idx2char[predicted_id])

        # Останавливаемся, если сгенерирован символ конца предложения
        if idx2char[predicted_id] in {'.', '!', '?'}:
            break

    return start_string + ''.join(text_generated)

# Генерация текста
start_string = "Чичиков произнес: "
generated_text = generate_text(model, start_string, num_generate=70, temperature=0.8)
print(generated_text)